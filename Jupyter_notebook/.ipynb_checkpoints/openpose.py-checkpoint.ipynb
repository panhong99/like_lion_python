{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74056aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 스켈레톤 그리기\n",
    "import cv2\n",
    "\n",
    "class motion_tracking:\n",
    "    def tracking_motion(self , image):\n",
    "        self.image = image\n",
    "        # MPII에서 각 파트 번호, 선으로 연결될 POSE_PAIRS\n",
    "        BODY_PARTS = { \"Head\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
    "                        \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
    "                        \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"Chest\": 14,\n",
    "                        \"Background\": 15 }\n",
    "\n",
    "        POSE_PAIRS = [ [\"Head\", \"Neck\"], [\"Neck\", \"RShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
    "                        [\"RElbow\", \"RWrist\"], [\"Neck\", \"LShoulder\"], [\"LShoulder\", \"LElbow\"],\n",
    "                        [\"LElbow\", \"LWrist\"], [\"Neck\", \"Chest\"], [\"Chest\", \"RHip\"], [\"RHip\", \"RKnee\"],\n",
    "                        [\"RKnee\", \"RAnkle\"], [\"Chest\", \"LHip\"], [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"] ]\n",
    "            \n",
    "        # 각 파일 path\n",
    "        protoFile = \"/Users/panhong/Desktop/openpose_sample/pose_deploy_linevec_faster_4_stages.prototxt\"\n",
    "        weightsFile = \"/Users/panhong/Desktop/openpose_sample/pose_iter_160000.caffemodel\"\n",
    "        \n",
    "        # 위의 path에 있는 network 불러오기\n",
    "\n",
    "        # 1번 인수는 .protext파일의 경로 \n",
    "        # 2번 인수는 학습된 모델이 있는 .caffemodel파일의 경로\n",
    "        net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
    "\n",
    "        # opencv를 이용하여 이미지 읽어오기\n",
    "        # image = image\n",
    "\n",
    "        # frame.shape = 불러온 이미지에서 height, width, color 받아옴\n",
    "        # 1 = 이미지의 높이 , 2 = 이미지의 너비 , 3 = 이미지의 채널\n",
    "        imageHeight, imageWidth, _ = image.shape\n",
    "\n",
    "\n",
    "        # network에 넣기위해 전처리\n",
    "        # blob = 동일한 방식으로 전처리된 동일한 너비 , 높이 , 채널 수를 가진 하나 이상의 이미지\n",
    "        # 변수 설명\n",
    "        # image = 입력 영상 , 이미지\n",
    "        # scalefavtor = 입력 영상 픽셀 값에 곱할 값 , 기본값은 1 \n",
    "        # size = 출력 영상의 크기(이 코드에서는 지정해놓은 weight , heigth의 값)\n",
    "        # mean = 입력영상 각 채널에서 뺼 평균 값 , 기본값은 (0,0,0)\n",
    "        # swapRB = R 과 B채널을 서로 바꿀것인지 결정하는 플래그 , 기본값은 False\n",
    "        # crop = 크롭 수행 여부 , 기본값은 False\n",
    "        inpBlob = cv2.dnn.blobFromImage(image, 1.0 / 255, (imageWidth, imageHeight), (0, 0, 0), swapRB=False, crop=False)\n",
    "        \n",
    "        # network에 넣어주기\n",
    "        # 가져온 이미지를 입력\n",
    "        net.setInput(inpBlob)\n",
    "\n",
    "        # 결과 받아오기\n",
    "        # 매개변수를 입력해주지 않아서 해석하면 전체네트워크에 대해 \n",
    "        # 정방향 전달을 실행\n",
    "        output = net.forward()\n",
    "\n",
    "        # output.shape[0] = 이미지 ID, [1] = 출력 맵의 높이, [2] = 너비\n",
    "        H = output.shape[2]\n",
    "        W = output.shape[3]\n",
    "        print(\"이미지 ID : \", len(output[0]), \", H : \", output.shape[2], \", W : \",output.shape[3]) # 이미지 ID\n",
    "\n",
    "        # 키포인트 검출시 이미지에 그려줌\n",
    "        points = []\n",
    "        for i in range(0,15):\n",
    "            # 해당 신체부위 신뢰도 얻음.\n",
    "            probMap = output[0, i, :, :]\n",
    "        \n",
    "            # global 최대값 찾기\n",
    "            minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
    "\n",
    "            # 원래 이미지에 맞게 점 위치 변경\n",
    "            x = (imageWidth * point[0]) / W\n",
    "            y = (imageHeight * point[1]) / H\n",
    "\n",
    "            # 키포인트 검출한 결과가 0.1보다 크면(검출한곳이 위 BODY_PARTS랑 맞는 부위면) points에 추가, 검출했는데 부위가 없으면 None으로    \n",
    "            if prob > 0.1 :    \n",
    "                cv2.circle(image, (int(x), int(y)), 3, (0, 255, 255), thickness=-1, lineType=cv2.FILLED)       # circle(그릴곳, 원의 중심, 반지름, 색)\n",
    "                cv2.putText(image, \"{}\".format(i), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, lineType=cv2.LINE_AA)\n",
    "                points.append((int(x), int(y)))\n",
    "            else :\n",
    "                points.append(None)\n",
    "\n",
    "        # 점만 찍혀있는 이미지 출력\n",
    "        cv2.imshow(\"Output-Keypoints\",image)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "        # 이미지 복사\n",
    "        imageCopy = image\n",
    "\n",
    "        # 각 POSE_PAIRS별로 선 그어줌 (머리 - 목, 목 - 왼쪽어깨, ...)\n",
    "        for pair in POSE_PAIRS:\n",
    "            partA = pair[0]             # Head\n",
    "            partA = BODY_PARTS[partA]   # 0\n",
    "            partB = pair[1]             # Neck\n",
    "            partB = BODY_PARTS[partB]   # 1\n",
    "            \n",
    "            print(partA,\" 와 \", partB, \" 연결\\n\")\n",
    "            if points[partA] and points[partB]:\n",
    "                cv2.line(imageCopy, points[partA], points[partB], (255, 0, 0), 2)\n",
    "\n",
    "        # 라인이 그려진 이미지를 출력\n",
    "        cv2.imshow(\"Output-Keypoints\",imageCopy)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "'''============================================================================================================================='''\n",
    "# # 영상 스켈레톤 그리기 \n",
    "\n",
    "# import cv2 as cv \n",
    "# import numpy as np\n",
    "# import argparse\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# net = cv.dnn.readNetFromTensorflow(\"/Users/panhong/Desktop/human-pose/graph_opt.pb\")\n",
    "\n",
    "# inWidht = 368\n",
    "# inHeight = 368\n",
    "# thr = 0.2\n",
    "\n",
    "# BODY_PARTS = { \"Nose\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
    "#                \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
    "#                \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"REye\": 14,\n",
    "#                \"LEye\": 15, \"REar\": 16, \"LEar\": 17, \"Background\": 18 }\n",
    "\n",
    "# POSE_PAIRS = [ [\"Neck\", \"RShoulder\"], [\"Neck\", \"LShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
    "#                [\"RElbow\", \"RWrist\"], [\"LShoulder\", \"LElbow\"], [\"LElbow\", \"LWrist\"],\n",
    "#                [\"Neck\", \"RHip\"], [\"RHip\", \"RKnee\"], [\"RKnee\", \"RAnkle\"], [\"Neck\", \"LHip\"],\n",
    "#                [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"], [\"Neck\", \"Nose\"], [\"Nose\", \"REye\"],\n",
    "#                [\"REye\", \"REar\"], [\"Nose\", \"LEye\"], [\"LEye\", \"LEar\"] ]\n",
    "\n",
    "# cap = cv.VideoCapture(\"/Users/panhong/Desktop/baseball/오타니.mov\")\n",
    "# cap.set(3,800)\n",
    "# cap.set(4,800)\n",
    "\n",
    "# if not cap.isOpened():\n",
    "#     cap = cv.VideoCapture(0)\n",
    "# if not cap.isOpened():\n",
    "#     raise IOError(\"Cannot open Video\")\n",
    "    \n",
    "# while cv.waitKey(1) < 0:\n",
    "#     hasFrame , frame = cap.read()\n",
    "#     if not hasFrame:\n",
    "#         cv.waitKey()\n",
    "#         break\n",
    "        \n",
    "#     frameWidth = frame.shape[1]\n",
    "#     frameHeight = frame.shape[0]\n",
    "#     net.setInput(cv.dnn.blobFromImage(frame , 1.0 , (inWidht , inHeight) , (127.5 , 127.5 , 127.5) , swapRB = True , crop = False))\n",
    "#     out = net.forward()\n",
    "#     out = out[:, :19, :, :]  # MobileNet output [1, 57, -1, -1], we only need the first 19 elements\n",
    "\n",
    "#     assert(len(BODY_PARTS) == out.shape[1])\n",
    "\n",
    "#     points = []\n",
    "    \n",
    "#     for i in range(len(BODY_PARTS)):\n",
    "#         # Slice heatmap of corresponging body's part.\n",
    "#         heatMap = out[0, i, :, :]\n",
    "\n",
    "#         # Originally, we try to find all the local maximums. To simplify a sample\n",
    "#         # we just find a global one. However only a single pose at the same time\n",
    "#         # could be detected this way.\n",
    "#         _, conf, _, point = cv.minMaxLoc(heatMap)\n",
    "#         x = (frameWidth * point[0]) / out.shape[3]\n",
    "#         y = (frameHeight * point[1]) / out.shape[2]\n",
    "#         # Add a point if it's confidence is higher than threshold.\n",
    "#         points.append((int(x), int(y)) if conf > thr else None)\n",
    "\n",
    "#     for pair in POSE_PAIRS:\n",
    "#         partFrom = pair[0]\n",
    "#         partTo = pair[1]\n",
    "#         assert(partFrom in BODY_PARTS)\n",
    "#         assert(partTo in BODY_PARTS)\n",
    "\n",
    "#         idFrom = BODY_PARTS[partFrom]\n",
    "#         idTo = BODY_PARTS[partTo]\n",
    "\n",
    "#         if points[idFrom] and points[idTo]:\n",
    "#             cv.line(frame, points[idFrom], points[idTo], (0, 255, 0), 3)\n",
    "#             cv.ellipse(frame, points[idFrom], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "#             cv.ellipse(frame, points[idTo], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "\n",
    "#     t, _ = net.getPerfProfile()\n",
    "#     freq = cv.getTickFrequency() / 1000\n",
    "#     cv.putText(frame, '%.2fms' % (t / freq), (10, 20), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0))\n",
    "    \n",
    "#     cv.imshow(\"pose estimation Tutorial\" , frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python39564bitbasecondaff88d2da3a154ebfb639a042a46a85ea"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
