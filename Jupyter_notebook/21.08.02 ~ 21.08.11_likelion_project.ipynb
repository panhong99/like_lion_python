{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bbf36a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d27e2fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : 513 , y : 238 , w : 665 , h : 721\n",
      "cropped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 마우스를 이용해서 관심영역 설정 후 이미지 저장\n",
    "isDragging = False\n",
    "x0 , y0 , w , h = -1 , -1 , -1 , -1\n",
    "blue , red = (255 , 0 , 0) , (0 , 0 , 255)\n",
    "\n",
    "def onMouse(event , x , y , flags , param):\n",
    "    global isDragging , x0 , y0 , img \n",
    "    # 마우스 왼쪽버튼 클릭시\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        isDragging = True\n",
    "        x0 = x\n",
    "        y0 = y\n",
    "    # 왼쪽 버튼을 누를채로 마우스 움직일시\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if isDragging:\n",
    "            # 이미지를 카피\n",
    "            img_draw = img.copy()\n",
    "            # x = x축 , y = y축 , w = width , h = height\n",
    "            # rentangle = 도형그리기 함수(매개변수들은 도형을 그릴 이미지 , 도형의 x , y , w , h값 , 색상이다.)\n",
    "            cv2.rectangle(img_draw , (x0 , y0) , (x , y) , blue , 2)\n",
    "            \n",
    "            # 슬라이싱당한 image의 원본을 출력\n",
    "            cv2.imshow(\"img\" , img_draw)\n",
    "            \n",
    "    # 마우스 왼쪽 버튼에서 손을 땠을떄\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        # 드래깅을 False로 변경 , 그리기 금지\n",
    "        if isDragging:\n",
    "            isDragging = False\n",
    "            w = x - x0\n",
    "            h = y - y0\n",
    "            # 드래깅이 완료되면 베이스 이미지에서 드래깅된 구역의 x , y , w , h 값을 출력해줌\n",
    "            print(\"x : %d , y : %d , w : %d , h : %d\" %(x0 , y0 , w , h))\n",
    "            # 드래깅이 제대로 되었다면 당연히  w , h는 0보다 클수밖에 없다.\n",
    "            if w > 0 and h > 0:\n",
    "                # 이미지를 카피\n",
    "                img_draw = img.copy()\n",
    "                # 이미지에 슬라이싱할 부위에 사각형 그려주기\n",
    "                cv2.rectangle(img_draw , (x0 , y0) , (x , y) , red , 2)\n",
    "                # 기본 베이스 이미지 출력\n",
    "                cv2.imshow(\"img\" , img_draw)\n",
    "                # roi = 원하는 이미지의 부분(사용자가 직접 슬라이싱한 부분)\n",
    "                roi = img[y0 : y0 + h , x0 : x0 + w]\n",
    "                # 슬라이싱한 이미지를 출력\n",
    "                cv2.imshow(\"cropped\" , roi)\n",
    "                # 슬라이싱한 이미지가 출력될 윈도우의 위치를 설정 \n",
    "                cv2.moveWindow(\"cropped\" , 0 , 0)\n",
    "                # 이미지 저장\n",
    "                cv2.imwrite(\"/Users/panhong/Desktop/baseball/cropped.jpg\" , roi)\n",
    "                print(\"cropped\")\n",
    "            else:\n",
    "                # 똑바로 드래깅을 못했을시\n",
    "                cv2.imshow(\"img\" , img)\n",
    "                print(\"좌측 상단에서 우측 하단으로 영역을 드래그 하세요.\")\n",
    "                \n",
    "# 읽을 이미지 불러오기\n",
    "img = cv2.imread(\"/Users/panhong/Desktop/baseball/오타니.png\")\n",
    "# 불러온 이미지 출력\n",
    "cv2.imshow(\"img\" , img)\n",
    "\n",
    "# onMouse에 매개변수는 x , y , flags , parms가 필요한데 \n",
    "# setMousecallback 함수에는 해당 내용들이 입력이 되므로 onMouse함수를 이용할 수가 있음\n",
    "cv2.setMouseCallback(\"img\" , onMouse)\n",
    "\n",
    "# 사용자가 슬라이싱한 이미지를 출력\n",
    "img_2 = cv2.imread(\"/Users/panhong/Desktop/baseball/cropped.jpg\")\n",
    "cv2.imshow(\"tracking\" , img_2)\n",
    "\n",
    "# 생선된 윈도우 전부제거\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "cv2.waitKey(1)\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe8035ea",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 ID :  44 , H :  89 , W :  80\n",
      "0  와  1  연결\n",
      "\n",
      "1  와  2  연결\n",
      "\n",
      "2  와  3  연결\n",
      "\n",
      "3  와  4  연결\n",
      "\n",
      "1  와  5  연결\n",
      "\n",
      "5  와  6  연결\n",
      "\n",
      "6  와  7  연결\n",
      "\n",
      "1  와  14  연결\n",
      "\n",
      "14  와  8  연결\n",
      "\n",
      "8  와  9  연결\n",
      "\n",
      "9  와  10  연결\n",
      "\n",
      "14  와  11  연결\n",
      "\n",
      "11  와  12  연결\n",
      "\n",
      "12  와  13  연결\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openpose import motion_tracking\n",
    "import cv2\n",
    "\n",
    "# 기존에 작성해놓은 motion tracking class를 이용해서 상단에 cropped된 이미지를 읽어서\n",
    "# 관절부위에 점을 찍고 라인을 그어줌\n",
    "tracking = motion_tracking()\n",
    "img = cv2.imread(\"/Users/panhong/Desktop/baseball/cropped.jpg\")\n",
    "tracking.tracking_motion(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d5d0ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(692, 522, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 상단 마우스로 처리한 부분을 마우스로 하지않고 코딩을 이용해서 원하는 부위를 설정\n",
    "img = cv2.imread(\"/Users/panhong/Desktop/baseball/오타니.png\")\n",
    "\n",
    "# x , y , w , h == 이미지내에서 슬라이싱을 원하는부위의 좌표값\n",
    "x = 592 \n",
    "y = 248\n",
    "w = 522\n",
    "h = 692\n",
    "\n",
    "# roi == 원하는 위치\n",
    "roi = img[y : y + h , x : x + w]\n",
    "\n",
    "print(roi.shape)\n",
    "\n",
    "# 설정한 위치에 사각형 도형을 그려줌\n",
    "cv2.rectangle(roi , (0 , 0) , (h - 1 , w - 1) , (0 , 0 , 255))\n",
    "\n",
    "# 이미지 출력\n",
    "cv2.imshow(\"img\" , img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "cv2.waitKey(1)\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0893fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 해당되는 영상을 흑백처리\n",
    "import numpy as np , cv2\n",
    "# 비디오 불러오기\n",
    "cap = cv2.VideoCapture(\"/Users/panhong/Desktop/baseball/오타니.mov\")\n",
    "# 프레임 레이트(영상진행속도) 가져오기 \n",
    "# 1초동안 가져올 프레임의 개수를 설정\n",
    "# 가져오는 프레임의 숫자가 작을수록 슬로우모션으로 영상이 진행\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# 해당연산은 영상의 프레임사이의 간격을 볼 수 확인할수 있다.\n",
    "# 예를 들어 해당식을 연산해서 delay를 print해보았을때 출력값이 30이라면 \n",
    "# 해당 영상의 1초당 출력하는 프레임은 30개라는 뜻이됩니다. \n",
    "delay = int(1000/fps)\n",
    "\n",
    "# 영상의 배경을 지워주는 함수 (움직이는 객체는 흰색으로 표시되며 배경만 있는\n",
    "# 영상이나 이미지일 경우는 검은색 화면만 출력됨)\n",
    "# 주로 cctv에서 객체의 움직임을 확인할때 사용함\n",
    "\n",
    "# SubtractorMOG2 , SubtractorGMG 함수종류가 있음 \n",
    "# 대부분 비슷하지만 약간의 차이점은 존재함\n",
    "fgbg = cv2.bgsegm.createBackgroundSubtractorMOG()\n",
    "\n",
    "# isOpened함수는 해당 영상이 제대로 열렸는지 확인을 할 수 있는함수\n",
    "# isOpened함수와 조건문을 이용해서 더 정확하게 영상이\n",
    "# 제대로 열렸는지 확인이 가능함\n",
    "while cap.isOpened():\n",
    "    # 영상이 제대로 열렸다면 ret의 값은 True가 됩니다.\n",
    "    # frame = 영상의 프레임을 가져옵니다.\n",
    "    ret , frame = cap.read()\n",
    "    # ret = False라면 해당 반복문을 중단시킵니다.\n",
    "    if not ret:\n",
    "        break\n",
    "    # apply함수도 마찬가지로 영상의 정적인 부분을 흑색으로 변화시키고\n",
    "    # 동적인 부분은 흰색으로 남기는 함수임\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    # 변색되지 않은 온전한 영상 출력\n",
    "    cv2.imshow(\"frame\" , frame)\n",
    "    # 동적인 부분만 흰색으로 출력되게 설정해준 영상을 출력\n",
    "    cv2.imshow(\"bgsub\" , fgmask)\n",
    "    # waitKey함수를 이용해서 키보드 이벤트 감지\n",
    "    # 아무버튼이나 이벤트가 발생하면 반복문 중단\n",
    "    if cv2.waitKey(1) & 0xff == 27:\n",
    "        break\n",
    "        \n",
    "# release함수를 이용해서 불러온 영상의파일 , 캡처 장치를 닫아줌 \n",
    "# 닫아주지 않을지 백그라운드에서 계속 작동중이므로 \n",
    "# 작업에 방해를 주는 요인으로 작용됨\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "cv2.waitKey(1)\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abe43de9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 ID :  44 , H :  89 , W :  79\n",
      "0  와  1  연결\n",
      "\n",
      "1  와  2  연결\n",
      "\n",
      "2  와  3  연결\n",
      "\n",
      "3  와  4  연결\n",
      "\n",
      "1  와  5  연결\n",
      "\n",
      "5  와  6  연결\n",
      "\n",
      "6  와  7  연결\n",
      "\n",
      "1  와  14  연결\n",
      "\n",
      "14  와  8  연결\n",
      "\n",
      "8  와  9  연결\n",
      "\n",
      "9  와  10  연결\n",
      "\n",
      "14  와  11  연결\n",
      "\n",
      "11  와  12  연결\n",
      "\n",
      "12  와  13  연결\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'============================================================================================================================='"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이미지 스켈레톤 그리기\n",
    "import cv2\n",
    "\n",
    "# 이미지 , 영상을 분석해서 적절한 위치의 관절 포인트의 번호를 \n",
    "# 지정해준 딕셔너리 전신부터 배경까지 총 15번까지의 포인트 번호\n",
    "BODY_PARTS = { \"Head\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
    "                \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
    "                \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"Chest\": 14,\n",
    "                \"Background\": 15 }\n",
    "\n",
    "# 최종출력시 관절들의 위치를 점으로 찍고 점을 연결시켜주어야 하므로 \n",
    "# 라인이 중구난방하게 그어지지 않게 관절의 연결도를 각각 리스트로 묶어놓았음 \n",
    "# [머리 , 목 ] , [목 , 좌측어깨] , [목 , 우측어깨]\n",
    "POSE_PAIRS = [ [\"Head\", \"Neck\"], [\"Neck\", \"RShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
    "                [\"RElbow\", \"RWrist\"], [\"Neck\", \"LShoulder\"], [\"LShoulder\", \"LElbow\"],\n",
    "                [\"LElbow\", \"LWrist\"], [\"Neck\", \"Chest\"], [\"Chest\", \"RHip\"], [\"RHip\", \"RKnee\"],\n",
    "                [\"RKnee\", \"RAnkle\"], [\"Chest\", \"LHip\"], [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"] ]\n",
    "    \n",
    "# prototext = 네트워크 구조가 담긴 파일\n",
    "# caffe model은 prototext 파일로 학습이 되고 학습된 모델의 이름이 \n",
    "# caffemodel이 된다. \n",
    "# caffe에서 사용할 protobuf들의 정의는 caffe.proto에서 확인가능\n",
    "protoFile = \"/Users/panhong/Desktop/openpose_sample/pose_deploy_linevec_faster_4_stages.prototxt\"\n",
    "\n",
    "# caffemodel = prototext의 네트워크로 학습 시킨 모델\n",
    "# 가중치와 바이어스가 담겨있음 \n",
    "# 일반적으로는 볼 수 없는 파일이지만 caffe 플랫폼으로는 개방해서 수정이 가능하다.\n",
    "weightsFile = \"/Users/panhong/Desktop/openpose_sample/pose_iter_160000.caffemodel\"\n",
    " \n",
    "# 위의 path에 있는 network 불러오기\n",
    "\n",
    "# 1번 인수는 .protext파일의 경로 \n",
    "# 2번 인수는 학습된 모델이 있는 .caffemodel파일의 경로\n",
    "# net은 변수이지만 의미가 있다.\n",
    "# net은 plaintext 모델링 언어(protext)로 정의된 레이어들과 그 연결들의 집합이다.\n",
    "# readNetFromCaffe = 네트워크를 불러오는 함수이다. \n",
    "# opencv를 이용해서 딥러닝을 하기 위해서는 dnn_Net클래스 객체 생성이 필요한데\n",
    "# 객체 생성에는 훈련된 가중치와 네트워크 구성을 저장하고 있는 파일이 필요함\n",
    "net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
    "\n",
    "# opencv를 이용하여 이미지 읽어오기\n",
    "image = cv2.imread(\"/Users/panhong/Desktop/baseball/cropped.jpg\")\n",
    "\n",
    "# frame.shape = 불러온 이미지에서 height, width, color 받아옴\n",
    "# 1 = 이미지의 높이 , 2 = 이미지의 너비 , 3 = 이미지의 채널\n",
    "imageHeight, imageWidth, _ = image.shape\n",
    "\n",
    "\n",
    "# network에 넣기위해 전처리\n",
    "# blob = 동일한 방식으로 전처리된 동일한 너비 , 높이 , 채널 수를 가진 하나 이상의 이미지\n",
    "# 변수 설명\n",
    "# image = 이미지 자체\n",
    "# 2 번째 인수 = 각 픽셀값의 배율을 지정해줌\n",
    "# 3 번째 인수 = 네트워크에 대한 기본 입력은 320 x 320임 따라서 blob를 만들려면 \n",
    "# 지정 할 필요가 있음 , 다른 차원 입력으로도 실험이 가능\n",
    "# size = 출력 영상의 크기(이 코드에서는 지정해놓은 weight , heigth의 값)\n",
    "# mean = 입력영상 각 채널에서 뺼 평균 값 , 기본값은 (0,0,0)\n",
    "# swapRB = R 과 B채널을 서로 바꿀것인지 결정하는 플래그 , 기본값은 False\n",
    "# 바꿀것인지 결정하는 이유는 opencv = bgr형식을 사용하지만 tensorflow = rgb형식을 사용하기 때문에 물어본것임\n",
    "# 마지막 매개변수는 이미지를 자르고 잘래낸 중심을 가질것이냐의 의미(???)\n",
    "# blob의 인자들을 입력할 때 모델 파일이 어떻게 학습되었는지 파악하고 그에 맞게 입력을 해주어여 함\n",
    "inpBlob = cv2.dnn.blobFromImage(image, 1.0 / 255, (imageWidth, imageHeight), (0, 0, 0), swapRB=False, crop=False)\n",
    " \n",
    "# network에 넣어주기\n",
    "# 가져온 이미지를 입력\n",
    "net.setInput(inpBlob)\n",
    "\n",
    "# 결과 받아오기\n",
    "# 매개변수를 입력해주지 않아서 해석하면 전체네트워크에 대해 \n",
    "# 정방향 전달을 실행\n",
    "# 상단에 blob를 통해서 이미지를 구성했고 \n",
    "# setInput을 이용해 net에다가 할당해주었다.\n",
    "# 그 이미지를 output변수가 가진것임 \n",
    "output = net.forward()\n",
    "\n",
    "# output.shape[0] = 이미지 ID, [1] = 출력 맵의 높이, [2] = 너비\n",
    "# H = height\n",
    "H = output.shape[2]\n",
    "# W = width\n",
    "W = output.shape[3]\n",
    "print(\"이미지 ID : \", len(output[0]), \", H : \", output.shape[2], \", W : \",output.shape[3]) # 이미지 ID\n",
    "\n",
    "# 키포인트 검출시 이미지에 그려줌\n",
    "points = []\n",
    "# 0 ~ 15 까지는 상단의 BODY_PARTS의 개수와 동일함\n",
    "for i in range(0,15):\n",
    "    # 해당 신체부위 신뢰도 얻음.\n",
    "    probMap = output[0, i, :, :]\n",
    "     \n",
    "    # 템플릿 매칭 = 어떤 이미지에서 부분이미지를 검색하고 찾는 방법\n",
    "    # minMaxLoc함수를 이용해서 비교되는 이미지와의 최소 , 최대포인터 , 최소 , 최대 지점의 값을 받을 수 있다.\n",
    "    # 최소 포인터의 값을 이용해서 해당되는 부분과의 유사도가 가장 낮을때를 알 수 있다.\n",
    "    # 최대 포인터를 이용하면 자연스럽게 가장 유사도가 높은 부분을 알 수 있다.\n",
    "    # global 최대값 찾기 = 비교이미지와 가장 매칭도가 높은곳 찾기\n",
    "    minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
    "\n",
    "    # 원래 이미지에 맞게 점 위치 변경\n",
    "    x = (imageWidth * point[0]) / W\n",
    "    y = (imageHeight * point[1]) / H\n",
    "    \n",
    "    # 키포인트가 0.1이면 비교이미지와의 매치가 90% 이상이라는 뜻으로 \n",
    "    # 해당 부위에 BODY_PARTS의 number을 cv2.circle을 이용해서 점으로 찍어준다.\n",
    "    # 키포인트 검출한 결과가 0.1보다 크면(검출한곳이 위 BODY_PARTS랑 맞는 부위면) points에 추가, 검출했는데 부위가 없으면 None으로    \n",
    "    if prob > 0.1 :    \n",
    "        # circle(그릴곳, 원의 중심, 반지름, 색)\n",
    "        cv2.circle(image, (int(x), int(y)), 3, (0, 255, 255), thickness=-1, lineType=cv2.FILLED)       \n",
    "        # putText함수를 이용해서 먼저 점을 찍은다음에 매칭된지점에 알맞은 부위번호를 적어준다.\n",
    "        cv2.putText(image, \"{}\".format(i), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, lineType=cv2.LINE_AA)\n",
    "        # \n",
    "        points.append((int(x), int(y)))\n",
    "    else :\n",
    "        points.append(None)\n",
    "\n",
    "# 점만 찍혀있는 이미지 출력\n",
    "cv2.imshow(\"Output-Keypoints\",image)\n",
    "# waitKey를 이용해서 다음 이미지로 점프\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# 이미지 복사\n",
    "imageCopy = image\n",
    "\n",
    "# 각 POSE_PAIRS별로 선 그어줌 (머리 - 목, 목 - 왼쪽어깨, ...)\n",
    "for pair in POSE_PAIRS:\n",
    "    partA = pair[0]             # Head\n",
    "    partA = BODY_PARTS[partA]   # 0\n",
    "    partB = pair[1]             # Neck\n",
    "    partB = BODY_PARTS[partB]   # 1\n",
    "    \n",
    "    print(partA,\" 와 \", partB, \" 연결\\n\")\n",
    "    # 두 part 모두 값이 할당이 되면 copy이미지에 라인을 그려줌\n",
    "    if points[partA] and points[partB]:\n",
    "        # img = 이미지 , points = 시작점 , 종료점 좌표 , color , 라인의 종류\n",
    "        cv2.line(imageCopy, points[partA], points[partB], (255, 0, 0), 2)\n",
    "\n",
    "# 라인이 그려진 이미지를 출력\n",
    "cv2.imshow(\"Output-Keypoints\",imageCopy)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "\n",
    "\n",
    "'''============================================================================================================================='''\n",
    "# 영상 스켈레톤 그리기 \n",
    "\n",
    "import cv2 as cv \n",
    "import numpy as np\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "net = cv.dnn.readNetFromTensorflow(\"/Users/panhong/Desktop/human-pose/graph_opt.pb\")\n",
    "\n",
    "inWidht = 368\n",
    "inHeight = 368\n",
    "thr = 0.2\n",
    "\n",
    "BODY_PARTS = { \"Nose\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
    "               \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
    "               \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"REye\": 14,\n",
    "               \"LEye\": 15, \"REar\": 16, \"LEar\": 17, \"Background\": 18 }\n",
    "\n",
    "POSE_PAIRS = [ [\"Neck\", \"RShoulder\"], [\"Neck\", \"LShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
    "               [\"RElbow\", \"RWrist\"], [\"LShoulder\", \"LElbow\"], [\"LElbow\", \"LWrist\"],\n",
    "               [\"Neck\", \"RHip\"], [\"RHip\", \"RKnee\"], [\"RKnee\", \"RAnkle\"], [\"Neck\", \"LHip\"],\n",
    "               [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"], [\"Neck\", \"Nose\"], [\"Nose\", \"REye\"],\n",
    "               [\"REye\", \"REar\"], [\"Nose\", \"LEye\"], [\"LEye\", \"LEar\"] ]\n",
    "\n",
    "cap = cv.VideoCapture(\"/Users/panhong/Desktop/baseball/오타니.mov\")\n",
    "cap.set(3,800)\n",
    "cap.set(4,800)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    cap = cv.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open Video\")\n",
    "    \n",
    "while cv.waitKey(1) < 0:\n",
    "    hasFrame , frame = cap.read()\n",
    "    if not hasFrame:\n",
    "        cv.waitKey()\n",
    "        break\n",
    "        \n",
    "    frameWidth = frame.shape[1]\n",
    "    frameHeight = frame.shape[0]\n",
    "    net.setInput(cv.dnn.blobFromImage(frame , 1.0 , (inWidht , inHeight) , (127.5 , 127.5 , 127.5) , swapRB = True , crop = False))\n",
    "    out = net.forward()\n",
    "    out = out[:, :19, :, :]  # MobileNet output [1, 57, -1, -1], we only need the first 19 elements\n",
    "\n",
    "    assert(len(BODY_PARTS) == out.shape[1])\n",
    "\n",
    "    points = []\n",
    "    \n",
    "    for i in range(len(BODY_PARTS)):\n",
    "        # Slice heatmap of corresponging body's part.\n",
    "        heatMap = out[0, i, :, :]\n",
    "\n",
    "        # Originally, we try to find all the local maximums. To simplify a sample\n",
    "        # we just find a global one. However only a single pose at the same time\n",
    "        # could be detected this way.\n",
    "        _, conf, _, point = cv.minMaxLoc(heatMap)\n",
    "        x = (frameWidth * point[0]) / out.shape[3]\n",
    "        y = (frameHeight * point[1]) / out.shape[2]\n",
    "        # Add a point if it's confidence is higher than threshold.\n",
    "        points.append((int(x), int(y)) if conf > thr else None)\n",
    "\n",
    "    for pair in POSE_PAIRS:\n",
    "        partFrom = pair[0]\n",
    "        partTo = pair[1]\n",
    "        assert(partFrom in BODY_PARTS)\n",
    "        assert(partTo in BODY_PARTS)\n",
    "\n",
    "        idFrom = BODY_PARTS[partFrom]\n",
    "        idTo = BODY_PARTS[partTo]\n",
    "\n",
    "        if points[idFrom] and points[idTo]:\n",
    "            cv.line(frame, points[idFrom], points[idTo], (0, 255, 0), 3)\n",
    "            cv.ellipse(frame, points[idFrom], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "            cv.ellipse(frame, points[idTo], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "\n",
    "    t, _ = net.getPerfProfile()\n",
    "    freq = cv.getTickFrequency() / 1000\n",
    "    cv.putText(frame, '%.2fms' % (t / freq), (10, 20), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0))\n",
    "    \n",
    "    cv.imshow(\"pose estimation Tutorial\" , frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e46318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python39564bitbasecondaff88d2da3a154ebfb639a042a46a85ea"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
